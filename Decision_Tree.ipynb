{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Decision Tree"
      ],
      "metadata": {
        "id": "mDFBs_S_AVPi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "**1. What is a Decision Tree, and how does it work**\n",
        "\n",
        "A Decision Tree is a flowchart-like structure used for classification and regression. It splits the dataset into subsets based on feature values, forming branches and leaf nodes. The model makes decisions by traversing from the root to a leaf node based on input features.\n",
        "\n",
        "---\n",
        "\n",
        "**2. What are impurity measures in Decision Trees**\n",
        "\n",
        "Impurity measures quantify the disorder or impurity in a dataset. They help decide where to split the data. Common impurity measures include Gini Impurity and Entropy. A node with all samples from one class has zero impurity, indicating a pure node.\n",
        "\n",
        "---\n",
        "\n",
        "**3. What is the mathematical formula for Gini Impurity**\n",
        "\n",
        "Gini Impurity is calculated as:\n",
        "**Gini = 1 - Σ (pᵢ)²**,\n",
        "where *pᵢ* is the probability of an element being classified to a particular class. It ranges from 0 (pure) to 0.5 (maximum impurity for binary classification).\n",
        "\n",
        "---\n",
        "\n",
        "**4. What is the mathematical formula for Entropy**\n",
        "\n",
        "Entropy is calculated using:\n",
        "**Entropy = -Σ pᵢ log₂(pᵢ)**,\n",
        "where *pᵢ* is the probability of a class. Higher entropy indicates more disorder. Entropy is 0 when the node is pure and maximum when all classes are equally probable.\n",
        "\n",
        "---\n",
        "\n",
        "**5. What is Information Gain, and how is it used in Decision Trees**\n",
        "\n",
        "Information Gain is the reduction in entropy after a dataset is split based on an attribute. It helps determine which feature to split on. The feature with the highest information gain is chosen at each node to improve model accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "**6. What is the difference between Gini Impurity and Entropy**\n",
        "\n",
        "Both measure impurity, but Gini is computationally simpler and slightly faster. Entropy involves logarithmic calculations and is more sensitive to class distribution. In practice, they produce similar trees, but Gini tends to create slightly better splits with faster computation.\n",
        "\n",
        "---\n",
        "\n",
        "**7. What is the mathematical explanation behind Decision Trees**\n",
        "\n",
        "Decision Trees use recursive binary splitting. At each node, a feature and threshold are chosen to maximize Information Gain or minimize Gini Impurity. The process continues until a stopping criterion is met, creating a model represented as a tree.\n",
        "\n",
        "---\n",
        "\n",
        "**8. What is Pre-Pruning in Decision Trees**\n",
        "\n",
        "Pre-Pruning stops the tree from growing once a condition is met, such as a minimum number of samples at a node or maximum depth. It helps prevent overfitting and reduces training time by simplifying the model during the building phase.\n",
        "\n",
        "---\n",
        "\n",
        "**9. What is Post-Pruning in Decision Trees**\n",
        "\n",
        "Post-Pruning first grows the full tree and then removes branches that add little value by evaluating performance on a validation set. It simplifies the tree after construction, improving generalization and reducing overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "**10. What is the difference between Pre-Pruning and Post-Pruning**\n",
        "\n",
        "Pre-Pruning stops tree growth early using conditions, while Post-Pruning grows the tree fully and trims back unnecessary branches. Pre-Pruning is faster, but may miss optimal splits. Post-Pruning is more accurate but computationally expensive.\n",
        "\n",
        "---\n",
        "\n",
        "**11. What is a Decision Tree Regressor**\n",
        "\n",
        "A Decision Tree Regressor predicts continuous values instead of classes. It splits the data to minimize variance in target values within nodes. Predictions are made by taking the average value of data points in the final leaf node.\n",
        "\n",
        "---\n",
        "\n",
        "**12. What are the advantages and disadvantages of Decision Trees**\n",
        "\n",
        "Advantages: easy to interpret, handle both numerical and categorical data, and require little data preprocessing. Disadvantages: prone to overfitting, unstable to small data changes, and can create complex trees without pruning.\n",
        "\n",
        "---\n",
        "\n",
        "**13. How does a Decision Tree handle missing values**\n",
        "\n",
        "Decision Trees can handle missing values by using surrogate splits (alternative features) or by assigning instances with missing values to the most frequent or weighted path based on training data distribution. However, preprocessing is often recommended.\n",
        "\n",
        "---\n",
        "\n",
        "**14. How does a Decision Tree handle categorical features**\n",
        "\n",
        "Decision Trees handle categorical features by splitting nodes based on category values. For nominal features, the tree evaluates all possible subsets for splitting. Categorical variables don't require encoding unless the implementation requires numeric input.\n",
        "\n",
        "---\n",
        "\n",
        "**15. What are some real-world applications of Decision Trees?**\n",
        "\n",
        "Decision Trees are used in loan approval, medical diagnosis, fraud detection, customer segmentation, and credit scoring. Their interpretability and flexibility make them suitable for scenarios requiring transparent decision-making and fast prediction.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vIDe4wv8AY9c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Practical"
      ],
      "metadata": {
        "id": "7G2crVK7A4Pq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#16 Write a Python program to train a Decision Tree Classifier on the Iris dataset and print the model accuracy\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "model = DecisionTreeClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "4girn-vdA8ip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#17 Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances\n",
        "model = DecisionTreeClassifier(criterion='gini')\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Feature importances:\", model.feature_importances_)\n"
      ],
      "metadata": {
        "id": "FQAPgUsnBPOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#18 Write a Python program to train a Decision Tree Classifier using Entropy as the splitting criterion and print the model accuracy\n",
        "model = DecisionTreeClassifier(criterion='entropy')\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "-UTxyczbBQzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#19 Write a Python program to train a Decision Tree Regressor on a housing dataset and evaluate using Mean Squared Error (MSE)\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "regressor = DecisionTreeRegressor()\n",
        "regressor.fit(X_train, y_train)\n",
        "y_pred = regressor.predict(X_test)\n",
        "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "ce1o3_J7BSS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#20 Write a Python program to train a Decision Tree Classifier and visualize the tree using graphviz\n",
        "from sklearn.tree import export_graphviz\n",
        "import graphviz\n",
        "\n",
        "dot_data = export_graphviz(model, out_file=None, feature_names=data.feature_names,\n",
        "                           class_names=data.target_names, filled=True, rounded=True,\n",
        "                           special_characters=True)\n",
        "graph = graphviz.Source(dot_data)\n",
        "graph.render(\"iris_tree\", format='png', cleanup=True)\n",
        "graph.view()\n"
      ],
      "metadata": {
        "id": "SE-zyYQgBT4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#21 Write a Python program to train a Decision Tree Classifier with a maximum depth of 3 and compare its accuracy with a fully grown tree\n",
        "model_full = DecisionTreeClassifier()\n",
        "model_full.fit(X_train, y_train)\n",
        "acc_full = accuracy_score(y_test, model_full.predict(X_test))\n",
        "\n",
        "model_limited = DecisionTreeClassifier(max_depth=3)\n",
        "model_limited.fit(X_train, y_train)\n",
        "acc_limited = accuracy_score(y_test, model_limited.predict(X_test))\n",
        "\n",
        "print(\"Full Tree Accuracy:\", acc_full)\n",
        "print(\"Max Depth 3 Accuracy:\", acc_limited)\n"
      ],
      "metadata": {
        "id": "zeYmf9RLBV2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#22 Write a Python program to train a Decision Tree Classifier using min_samples_split=5 and compare its accuracy with a default tree\n",
        "model_default = DecisionTreeClassifier()\n",
        "model_default.fit(X_train, y_train)\n",
        "acc_default = accuracy_score(y_test, model_default.predict(X_test))\n",
        "\n",
        "model_split5 = DecisionTreeClassifier(min_samples_split=5)\n",
        "model_split5.fit(X_train, y_train)\n",
        "acc_split5 = accuracy_score(y_test, model_split5.predict(X_test))\n",
        "\n",
        "print(\"Default Tree Accuracy:\", acc_default)\n",
        "print(\"min_samples_split=5 Accuracy:\", acc_split5)\n"
      ],
      "metadata": {
        "id": "9w9XvDrRBkGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#23 Write a Python program to apply feature scaling before training a Decision Tree Classifier and compare its accuracy with unscaled data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_scaled = DecisionTreeClassifier()\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "acc_scaled = accuracy_score(y_test, model_scaled.predict(X_test_scaled))\n",
        "\n",
        "model_unscaled = DecisionTreeClassifier()\n",
        "model_unscaled.fit(X_train, y_train)\n",
        "acc_unscaled = accuracy_score(y_test, model_unscaled.predict(X_test))\n",
        "\n",
        "print(\"Scaled Accuracy:\", acc_scaled)\n",
        "print(\"Unscaled Accuracy:\", acc_unscaled)\n"
      ],
      "metadata": {
        "id": "cBS-qLs-BmId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#24 Write a Python program to train a Decision Tree Classifier using One-vs-Rest (OvR) strategy for multiclass classification\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "ovr_model = OneVsRestClassifier(DecisionTreeClassifier())\n",
        "ovr_model.fit(X_train, y_train)\n",
        "y_pred = ovr_model.predict(X_test)\n",
        "print(\"OvR Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "tA8v31FpBnj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#25 Write a Python program to train a Decision Tree Classifier and display the feature importance scores\n",
        "model = DecisionTreeClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Feature Importances:\", model.feature_importances_)\n"
      ],
      "metadata": {
        "id": "vCkfZCPyBpCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#26 Write a Python program to train a Decision Tree Regressor with max_depth=5 and compare its performance with an unrestricted tree\n",
        "reg_default = DecisionTreeRegressor()\n",
        "reg_default.fit(X_train, y_train)\n",
        "mse_default = mean_squared_error(y_test, reg_default.predict(X_test))\n",
        "\n",
        "reg_limited = DecisionTreeRegressor(max_depth=5)\n",
        "reg_limited.fit(X_train, y_train)\n",
        "mse_limited = mean_squared_error(y_test, reg_limited.predict(X_test))\n",
        "\n",
        "print(\"Unrestricted MSE:\", mse_default)\n",
        "print(\"Max Depth 5 MSE:\", mse_limited)\n"
      ],
      "metadata": {
        "id": "3cG-F_VvBqgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#27 Write a Python program to train a Decision Tree Classifier, apply Cost Complexity Pruning (CCP), and visualize its effect on accuracy\n",
        "path = model.cost_complexity_pruning_path(X_train, y_train)\n",
        "ccp_alphas = path.ccp_alphas[:-1]\n",
        "acc_scores = []\n",
        "\n",
        "for ccp_alpha in ccp_alphas:\n",
        "    clf = DecisionTreeClassifier(ccp_alpha=ccp_alpha)\n",
        "    clf.fit(X_train, y_train)\n",
        "    acc = accuracy_score(y_test, clf.predict(X_test))\n",
        "    acc_scores.append(acc)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(ccp_alphas, acc_scores)\n",
        "plt.xlabel(\"ccp_alpha\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Accuracy vs CCP Alpha\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "51smDNrSBsSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#28 Write a Python program to train a Decision Tree Classifier and evaluate its performance using Precision, Recall, and F1-Score\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Precision:\", precision_score(y_test, y_pred, average='macro'))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred, average='macro'))\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred, average='macro'))\n"
      ],
      "metadata": {
        "id": "_Ixt8GPYBzbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#29 Write a Python program to train a Decision Tree Classifier and visualize the confusion matrix using seaborn\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, cmap=\"Blues\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "OT2B6JjRB0_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#30 Write a Python program to train a Decision Tree Classifier and use GridSearchCV to find the optimal values for max_depth and min_samples_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [3, 5, 10, None],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "grid = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "print(\"Best Params:\", grid.best_params_)\n",
        "print(\"Best Score:\", grid.best_score_)\n"
      ],
      "metadata": {
        "id": "5T7gQjd9B3hl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}